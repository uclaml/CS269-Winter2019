

Testing

## Overview

Deep learning has achieved great success in many applications such as image processing, speech recognition and Go games. However, the reason why deep learning is so powerful remains elusive. The goal of this course is to understand the successes of deep learning by studying and building the theoretical foundations of deep learning. Topics covered by this course include but are not limited to: expressive power of deep learning, optimization for deep learning, generalization performance of deep learning and robustness of deep learning. Instructor will give lectures on advanced topics of statistical learning theory. Students will present and discuss papers on the selected topics, and do a course project.

## Prerequisites:
Two years of college mathematics, including calculus, linear algebra, probability and statistics, and the ability to write computer programs. CS 260 or an equivalent course.

## Logistics

<!--University of California, Los Angeles  -->

- Time: **Monday and Wednesday 4:00PM - 5:50PM**
- Location: **WGYOUNG 4216**  
- Instructor: [Quanquan Gu](http://web.cs.ucla.edu/~qgu/) (Email: qgu at cs dot ucla dot edu)   
- Teaching Assistant: Xinzhu Bei (Email: xzbei at cs dot ucla dor edu)
- Office hours: 
    - The instructor's office hour is Thursday 1:30pm-2:30pm at Engineering VI 282. 
    - The TA's office hour is Monday 11:00am-1:00pm at Bolter Hall 3256S-F.

## Recommended Textbook

There is no required textbook. The following are recommended textbooks:

1. Shai Shalev-Shwartz, and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge University Press, 2014. 
2. Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT press, 2012. 
3. Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning. Vol. 1. Cambridge: MIT press, 2016.
4. Aston Zhang, Zack C. Lipton, Mu Li, Alex J. Smola, Dive into Deep Learning, 2018.

## Grading Policy
 
Grades will be computed based on the following factors:

- Quiz 10%

- Scribing 10%

- Homework 30%

- Paper Presentation 20%

- Project 30%

## Schedule


| # | Date  | Topic  | pdf | ipynb  |
|----|----|----|----|----|
| | | **Part I: Statistical Learning Theory** | | |
| 1 | 1/7 |  Concentration Inequalities |  |  |
| 2 | 1/9 | Uniform Convergence |   | ---  |
| 3 | 1/14 | Rademacher Complexity |   | ---  |
| | | **Part II: Deep Learning Theory** | | |
| 10 | 2/6 | Generalization Error of DNN I | | |
